# Approximate-computing-on-Neural-Networks
Evaluating the impact of weighs accuracy reduction in a Neural Network over the Classification Accuracy

The amount of memory used to store the parameters of a neural network is dominated by storing weights. Therefore, reducing the number of bits to represent weights has a positive impact on use on the amount of resources required. The reduction in the number of bits also offers the possibility of using reduced arithmetic circuits with a consequent reduction in area, power and possibly a reduction in the critical path and therefore an increase in the clock frequency. The reduction in the number of bits used to represent weights certainly has an impact on the accuracy of the neural network. We want to evaluate this impact.
